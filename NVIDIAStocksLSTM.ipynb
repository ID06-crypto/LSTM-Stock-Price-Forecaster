{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpEoy-Z3NyaR",
        "outputId": "a8d94641-fdb2-4519-e1ba-66ac8ee133c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: IanD24\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/vishveshdumbre/nvidia-180-days-minute-dataset-2025\n",
            "Downloading nvidia-180-days-minute-dataset-2025.zip to ./nvidia-180-days-minute-dataset-2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.55M/1.55M [00:00<00:00, 684MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets --quiet\n",
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/vishveshdumbre/nvidia-180-days-minute-dataset-2025\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available()else 'cpu'"
      ],
      "metadata": {
        "id": "P80Asbc1OTL-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Stocks datasets from Kaggle\n",
        "# Clearning and Normalizing data by deviding by largest values in columns\n",
        "\n",
        "full_df = pd.read_csv(\"/content/nvidia-180-days-minute-dataset-2025/nvda_180_days_minute_data.csv\")\n",
        "train_df = full_df.iloc[: int(len(full_df) * 0.8)]\n",
        "test_df = full_df.iloc[int(len(full_df) * 0.8) :]\n",
        "\n",
        "train_df.dropna(inplace=True)\n",
        "test_df.dropna(inplace=True)\n",
        "\n",
        "train_df.drop(columns=['symbol', 'timestamp'], inplace=True)\n",
        "test_df.drop(columns=['symbol', 'timestamp'], inplace=True)\n",
        "\n",
        "train_df = train_df / train_df.max()\n",
        "test_df = test_df / test_df.max()\n",
        "\n",
        "# Creating Labels\n",
        "train_labels = train_df[\"open\"].shift(-1) / train_df[\"open\"]\n",
        "test_labels = test_df[\"open\"].shift(-10) / test_df[\"open\"]\n",
        "\n",
        "\n",
        "# Getting rid of last rows of each dataset\n",
        "train_labels = train_labels[:-1]\n",
        "test_labels = test_labels[:-1]\n",
        "\n",
        "train_df = train_df[:-1]\n",
        "test_df = test_df[:-1]\n",
        "\n",
        "\n",
        "# Turning it all into arrays\n",
        "train_df = np.array(train_df)\n",
        "test_df = np.array(test_df)\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhzi1aKzOZR5",
        "outputId": "f35ac99e-b49d-4fa6-a5a5-fa4062bc3b61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3943682080.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df.dropna(inplace=True)\n",
            "/tmp/ipython-input-3943682080.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df.dropna(inplace=True)\n",
            "/tmp/ipython-input-3943682080.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df.drop(columns=['symbol', 'timestamp'], inplace=True)\n",
            "/tmp/ipython-input-3943682080.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df.drop(columns=['symbol', 'timestamp'], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Dataset class to allow np arrarys to be converted to tensors for use with pytorch module\n",
        "class squential_dataset(Dataset):\n",
        "  def __init__(self, X, Y, sequence_length):\n",
        "    self.sequence_length = sequence_length\n",
        "    self.X = []\n",
        "    self.Y = []\n",
        "\n",
        "    for i in range(len(X) - sequence_length):\n",
        "      self.X.append(X[i : i+sequence_length])\n",
        "      self.Y.append(Y[i : i+sequence_length])\n",
        "\n",
        "    self.X = torch.tensor(self.X, dtype = torch.float).to(device)\n",
        "    self.Y = torch.tensor(self.Y, dtype = torch.float).to(device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.Y[index]"
      ],
      "metadata": {
        "id": "LPNu9drhPZow"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning data to variables for training, validation, and testing data\n",
        "\n",
        "sequence_length = 10\n",
        "\n",
        "training_data = squential_dataset(train_df[ : int(0.8 * len(train_df)), :], train_labels[ :int(0.8 * len(train_labels))], sequence_length)\n",
        "validation_data = squential_dataset(train_df[ int(0.8 * len(train_df)) : , :], train_labels[int(0.8 * len(train_labels)) : ], sequence_length)\n",
        "testing_data = squential_dataset(test_df, test_labels, sequence_length)"
      ],
      "metadata": {
        "id": "eVzLCASrSpLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240bb5ce-6f56-426b-af21-11ab9d8a5de2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-266683371.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  self.X = torch.tensor(self.X, dtype = torch.float).to(device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataloader iterables for batching data in training\n",
        "train_dataloader = DataLoader(training_data, batch_size = 16)\n",
        "validation_dataloader = DataLoader(validation_data, batch_size = 16)\n",
        "testing_dataloader = DataLoader(testing_data, batch_size = 16)"
      ],
      "metadata": {
        "id": "6LKZQhGVUP0x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting hyper paramaters for the network\n",
        "hidden_size = 5\n",
        "num_classes = 1\n",
        "num_epochs = 3\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 5\n",
        "num_layers = 2"
      ],
      "metadata": {
        "id": "GyxDleMS6lGA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buildingl class for my neural network including the types of layers and activation functions\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_stacked_layers = num_stacked_layers\n",
        "    self.LSTM = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    self.linear_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.num_stacked_layers, x.size(0), self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(self.num_stacked_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "    out, _ = self.LSTM(x, (h0, c0))\n",
        "    out = out[:, -1, :]\n",
        "    out = self.linear_layer(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "model = LSTM(input_size, hidden_size, num_layers).to(device)"
      ],
      "metadata": {
        "id": "WOx48juK6oWe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch():\n",
        "  model.train(True)\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for batch in train_dataloader:\n",
        "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "    output = model(x_batch)\n",
        "    loss = loss_function(output, y_batch.unsqueeze(1))\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def validate_epoch():\n",
        "  model.train(False)\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output = model(x_batch)\n",
        "      loss = loss_function(output, y_batch.unsqueeze(1))\n",
        "      running_loss += loss.item()\n",
        "\n",
        "  avg_loss = running_loss / len(validation_dataloader)\n",
        "\n",
        "  print(f\"Loss: {avg_loss} \")\n",
        "  print(\"------------------------\")\n"
      ],
      "metadata": {
        "id": "aH6XQRE2aXhr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr = 1e-3)\n",
        "epochs = 10\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_one_epoch()\n",
        "  validate_epoch()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5uZ0LDfbZy_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db4c6f2-0904-48fd-d88c-0c21d25ca9cd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([16, 1, 10])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([10, 1, 10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([12, 1, 10])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 3.4877810092753195e-06 \n",
            "------------------------\n",
            "Loss: 3.6674996213817114e-06 \n",
            "------------------------\n",
            "Loss: 5.325723083167884e-06 \n",
            "------------------------\n",
            "Loss: 4.044053473724792e-06 \n",
            "------------------------\n",
            "Loss: 2.2991685167703422e-06 \n",
            "------------------------\n",
            "Loss: 1.8408724098263452e-06 \n",
            "------------------------\n",
            "Loss: 1.759299688837886e-06 \n",
            "------------------------\n",
            "Loss: 1.6816684533793591e-06 \n",
            "------------------------\n",
            "Loss: 1.6156381910782514e-06 \n",
            "------------------------\n",
            "Loss: 1.5618656700559324e-06 \n",
            "------------------------\n"
          ]
        }
      ]
    }
  ]
}